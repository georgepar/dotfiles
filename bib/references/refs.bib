
@inproceedings{you2016image,
  title={Image captioning with semantic attention},
  author={Q. You et al.},
  booktitle={Proc. CVPR},
  organization={IEEE},
  year={2016}
}

@inproceedings{kumar2020gated,
  title={Gated mechanism for attention based multi modal sentiment analysis},
  author={Kumar, A. and Vepa, J.},
  booktitle={ICASSP},
  year={2020},
  organization={IEEE}
}

@article{sun2019multi,
  title={Multi-Modal Sentiment Analysis Using Deep Canonical Correlation Analysis},
  author={Sun, Z. and Sarma, P. K and Sethares, W. and Bucy, E. P},
  journal={Proc. Interspeech},
  year={2019}
}

@inproceedings{Huang2020MultimodalTF,
  title={Multimodal Transformer Fusion for Continuous Emotion Recognition},
  author={Huang, J. and Tao, J. and Liu, B. and Lian, Z. and Niu, M.},
  booktitle={ICASSP},
  year={2020},
  organization={IEEE}
}

@inproceedings{wang2019words,
  title={Words can shift: Dynamically adjusting word representations using nonverbal behaviors},
  author={Y. Wang et al.},
  booktitle={Proc. AAAI},
  year={2019}
}

@misc{yang2020mtgat,
      title={MTGAT: Multimodal Temporal Graph Attention Networks for Unaligned Human Multimodal Language Sequences}, 
      author={Yang, J. and Wang, Y. and Yi, R. and Zhu, Y. and Rehman, A. and Zadeh, A. and Poria, S. and Morency, L. P.},
      year={2020},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{liu2018efficient,
  title={Efficient Low-rank Multimodal Fusion With Modality-Specific Factors},
  author={Z. Liu et al.},
  booktitle={Proc. 56th ACL},
  year={2018}
}


@article{georgiou2019deep,
  title={Deep Hierarchical Fusion with Application in Sentiment Analysis},
  author={Georgiou, E. and Papaioannou, C. and Potamianos, A.},
  journal={Proc. Interspeech},
  year={2019}
}

@inproceedings{pham2019found,
  title={Found in translation: Learning robust joint representations by cyclic translations between modalities},
  author={H. Pham et al.},
  booktitle={Proc. AAAI},
  year={2019}
}

@inproceedings{wei2019mmgcn,
  title={MMGCN: Multi-modal graph convolution network for personalized recommendation of micro-video},
  author={Wei, Y. and Wang, X. and Nie, L. and He, X. and Hong, R. and Chua, T.S.},
  booktitle={Proc. 27th ACM},
  year={2019}
}

@inproceedings{ngiam2011multimodal,
  title={Multimodal deep learning},
  author={Ngiam, J. and Khosla, A. and Kim, M. and Nam, J. and Lee, H. and Ng, A. Y.},
  booktitle={ICML},
  year={2011}
}

@article{baltruvsaitis2018multimodal,
  title={Multimodal machine learning: A survey and taxonomy},
  author={Baltru{\v{s}}aitis, T. and Ahuja, C. and Morency, L. P.},
  journal={Transactions on Pattern Analysis and Machine Intelligence},
  year={2018},
  publisher={IEEE}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, S. and Schmidhuber, J.},
  journal={Neural computation},
  year={1997},
  publisher={MIT Press}
}

@inproceedings{adam14,
  author    = {Diederik P. K. and
               Jimmy B.},
  editor    = {Yoshua B. and
               Yann L.},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd ICLR},
  year      = {2015},
}

@inproceedings{capsules2017,
  author    = {Sabour, S. and
               Frosst, N. and
               Hinton, G. E.},
  title     = {Dynamic Routing Between Capsules},
  booktitle = {Proc. 30th NeurIps},
  year      = {2017},
}

@article{metallinou2012context,
  title={Context-sensitive learning for enhanced audiovisual emotion classification},
  author={A. Metallinou et al.},
  journal={Transactions on Affective Computing},
  year={2012},
  publisher={IEEE}
}

@inproceedings{vilbert2019,
  author    = {Lu, J. and
               Batra, D. and
               Parikh, D. and
               Lee, S.},
  title     = {ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations
               for Vision-and-Language Tasks},
  booktitle = {Proc. 32nd NeurIps},
  year      = {2019},
}

@inproceedings{zadeh2017tensor,
  title={Tensor Fusion Network for Multimodal Sentiment Analysis},
  author={A. Zadeh et al.},
  booktitle={Proc. EMNLP},
  year={2017}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={A. Vaswani et al.},
  booktitle={Proc. 31st NeurIps},
  year={2017}
}

@article{lee2011emotion,
  title={Emotion recognition using a hierarchical binary decision tree approach},
  author={C. C. Lee et al.},
  journal={Speech Communication},
  year={2011},
  publisher={Elsevier}
}

@inproceedings{gu2018multimodal,
  title={Multimodal affective analysis using hierarchical attention strategy with word-level alignment},
  author={Y. Gu et al.},
  booktitle={Proc. ACL},
  year={2018},
}

@article{wollmer2013lstm,
  title={LSTM-Modeling of continuous emotions in an audiovisual affect recognition framework},
  author={M. W{\"o}llmer et al.},
  journal={Image and Vision Computing},
  year={2013},
  publisher={Elsevier}
}

@inproceedings{rozgic2012ensemble,
  title={Ensemble of svm trees for multimodal emotion recognition},
  author={V. Rozgi{\'c} et al.},
  booktitle={Proc. APSIPA},
  year={2012},
  organization={IEEE}
}

@INPROCEEDINGS{poria-cmkl-16,
  author={Poria, S. and Chaturvedi, I. and Cambria, E. and Hussain, A.},
  booktitle={Proc. ICDM}, 
  title={Convolutional MKL Based Multimodal Emotion Recognition and Sentiment Analysis}, 
  year={2016},
  organization={IEEE}
}
  

@inproceedings{Srivastava12,
  author    = {Srivastava, N. and
               Salakhutdinov, R.},
  title     = {Multimodal Learning with Deep Boltzmann Machines},
  booktitle = {Proc. 26th NeurIps},
  year      = {2012},
}

@article{Mai_Hu_Xing_2020, 
title={Modality to Modality Translation: An Adversarial Representation Learning and Graph Fusion Network for Multimodal Fusion}, 
 journal={Proc. AAAI}, 
 author={Mai, S. and Hu, H. and Xing, S.},
 year={2020}
}

@article{Zadeh_Liang_Poria_Vij_Cambria_Morency_2018, 
title={Multi-attention Recurrent Network for Human Communication Comprehension}, 
journal={Proc. AAAI}, 
author={A. Zadeh et al.},
year={2018},
}

@article{Khare_2020,
   title={Multi-Modal Embeddings Using Multi-Task Learning for Emotion Recognition},

   journal={Proc. Interspeech},
   author={Khare, A. and Parthasarathy, S. and Sundaram, S.},
   year={2020}
}



@misc{mai2020analyzing,
      title={Analyzing Unaligned Multimodal Sequence via Graph Convolution and Graph Pooling Fusion}, 
      author={Mai, S. and Xing, S. and He, J. and Zeng, Y. and Hu, H.},
      year={2020},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{antol2015vqa,
  title={Vqa: Visual question answering},
  author={S. Antol et al.},
  booktitle={Proc. CVPR},
  year={2015},
  organizer={IEEE}
}

@inproceedings{anderson2018bottom,
  title={Bottom-up and top-down attention for image captioning and visual question answering},
  author={Anderson, P. and He, X. and Buehler, C. and Teney, D. and Johnson, M. and Gould, S. and Zhang, L.},
  booktitle={Proc. CVPR},
  year={2018},
  organizer={IEEE}
}

@article{KLEMEN2012111,
title = "Current perspectives and methods in studying neural mechanisms of multisensory interactions",
journal = "Neuroscience \& Biobehavioral Reviews",
year = "2012",
author = "Klemen, J. and Chambers, C. D.",
}

@article{neil2006development,
  title={Development of multisensory spatial integration and perception in humans},
  author={P. A. Neil et al.},
  journal={Developmental science},
  year={2006},
}

@article{bar2013top,
  title={Top-down effects in visual},
  author={Bar, M. and Bubic, A.},
  journal={The Oxford Handbook of Cognitive Neuroscience, Volume 2},
  year={2013}
}

@article{stroop1935studies,
  title={Studies of interference in serial verbal reactions.},
  author={Stroop, J R.},
  journal={Journal of experimental psychology},
  year={1935},
}

@article{lupyan2017objective,
  title={Objective effects of knowledge on visual perception.},
  author={Lupyan, G.},
  journal={Journal of experimental psychology: human perception and performance},
  year={2017}
}

@article{balcetis2010wishful,
  title={Wishful seeing: More desired objects are seen as closer},
  author={Balcetis, E. and Dunning, D.},
  journal={Psychological science},
  year={2010},
}

@article{proffitt1995perceiving,
  title={Perceiving geographical slant},
  author={Proffitt, D. R and Bhalla, M. and Gossweiler, R. and Midgett, J.},
  journal={Psychonomic bulletin \& review},
  year={1995},
  publisher={Springer}
}


@article{sohoglu2012predictive,
  title={Predictive top-down integration of prior knowledge during speech perception},
  author={Sohoglu, E. and Peelle, J. E and Carlyon, R. P and Davis, M. H},
  journal={Journal of Neuroscience},
  year={2012},
}

@article{TEUFEL201717,
title = "How to (and how not to) think about top-down influences on visual perception",
journal = "Consciousness and Cognition",
year = "2017",
author = "Teufel, C. and Nanay, B.",
}

@article{MANITA20151304,
title = "A Top-Down Cortical Circuit for Accurate Sensory Perception",
journal = "Neuron",
year = "2015",
author = "S. Manita et al.",
}

@article{firestone2014top,
  title={“Top-down” effects where none should be found: The El Greco fallacy in perception research},
  author={Firestone, C. and Scholl, B. J},
  journal={Psychological science},
  year={2014},
}

@article{houde2015cortical,
  title={The cortical computations underlying feedback control in vocal production},
  author={Houde, J. F and Chang, E. F},
  journal={Current opinion in neurobiology},
  year={2015},
}

@article{SHAFER2019112214,
title = "Visual feedback during motor performance is associated with increased complexity and adaptability of motor and neural output",
journal = "Behavioural Brain Research",
year = "2019",
author = "R. L. Shafer et al."
}

@inproceedings{degottex2014covarep,
  title={COVAREP—A collaborative voice analysis repository for speech technologies},
  author={Degottex, Gilles and Kane, John and Drugman, Thomas and Raitio, Tuomo and Scherer, Stefan},
  booktitle={ICASSP},
  year={2014},
  organization={IEEE}
}

 @misc{imotions_2016,
 title={Facial Expression Analysis Pocket Guide}, 
 url={https://imotions.com/guides/facial-expression-analysis/},
 journal={imotions}, author={iMotions}, year={2016}
 } 

@article{yuan2008speaker,
  title={Speaker identification on the SCOTUS corpus},
  author={Yuan, J. and Liberman, M.},
  journal={Journal of the Acoustical Society of America},
  year={2008},
}


@inproceedings{caglayan-etal-2019-probing,
    title = "Probing the Need for Visual Context in Multimodal Machine Translation",
    author = {Caglayan, O.  and
      Madhyastha, P.  and
      Specia, L.  and
      Barrault, L.},
    booktitle = "Proc. NAACL)",
    year = "2019",
}

@inproceedings{paraskevopoulos-etal-2020-multimodal,
    title = "Multimodal and Multiresolution Speech Recognition with Transformers",
    author = "Paraskevopoulos, G.  and
      Parthasarathy, S.  and
      Khare, A.  and
      Sundaram, S.",
    booktitle = "Proc. 58th ACL",
    year = "2020",
}

@inproceedings{srinivasan-etal-2020-multimodal,
    title = "Multimodal Speech Recognition with Unstructured Audio Masking",
    author = "Srinivasan, T.  and
      Sanabria, R.  and
      Metze, F.  and
      Elliott, D.",
    booktitle = "Proc. 1st Workshop on NLPBT",
    year = "2020",
}

@inproceedings{shenoy-sardana-2020-multilogue,
    title = "Multilogue-Net: A Context-Aware {RNN} for Multi-modal Emotion Detection and Sentiment Analysis in Conversation",
    author = "Shenoy, A.  and
      Sardana, A.",
    booktitle = "Proc. 2nd Challenge-HML",
    year = "2020",
}


@inproceedings{bagher-zadeh-etal-2018-multimodal,
    title = "Multimodal Language Analysis in the Wild: {CMU}-{MOSEI} Dataset and Interpretable Dynamic Fusion Graph",
    author = "A. Zadeh et al.",
    booktitle = "Proc. 56th ACL",
    year = "2018",
}


@inproceedings{tsai-etal-2019-multimodal,
    title = "Multimodal Transformer for Unaligned Multimodal Language Sequences",
    author = " Y. H. H. Tsai et al.",
    booktitle = "Proc. 57th ACL",
    year = "2019",
}


@inproceedings{delbrouck-etal-2020-transformer,
    title = "A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis",
    author = "Delbrouck, J. B.  and
      Tits, N.  and
      Brousmiche, M.  and
      Dupont, S.",
    booktitle = "2nd Challenge-HML",
    year = "2020",
}

@inproceedings{rahman-etal-2020-integrating,
    title = "Integrating Multimodal Information in Large Pretrained Transformers",
    author = "W. Rahman et al.",
    booktitle = "Proc. 58th ACL",
    year = "2020",
}

@article{wen2021cross,
  title={Cross-modal context-gated convolution for multi-modal sentiment analysis},
  author={Wen, H. and You, S. and Fu, Y.},
  journal={Pattern Recognition Letters},
  year={2021},
}

@inproceedings{sourav-ouyang-2021-lightweight,
    title = "Lightweight Models for Multimodal Sequential Data",
    author = "Sourav, S.  and
      Ouyang, J.",
    booktitle = "Proc. 11th WASSA",
    year = "2021",
}

@inproceedings{pennington-etal-2014-glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, J.  and
      Socher, R.  and
      Manning, C.",
    booktitle = "Proc. EMNLP",
    year = "2014",
}

@inproceedings{tsai-etal-2020-multimodal,
    title = "Multimodal Routing: Improving Local and Global Interpretability of Multimodal Language Analysis",
    author = "Y. H. H. Tsai et al.",
    booktitle = "Proc. EMNLP",
    year = "2020",
}

@article{BENEDETTO2015352,
title = {Rapid serial visual presentation in reading: The case of Spritz},
journal = {Computers in Human Behavior},
volume = {45},
pages = {352-358},
year = {2015},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2014.12.043},
url = {https://www.sciencedirect.com/science/article/pii/S0747563214007663},
author = {Simone Benedetto and Andrea Carbone and Marco Pedrotti and Kevin {Le Fevre} and Linda Amel Yahia Bey and Thierry Baccino},
keywords = {Reading, Rapid serial visual presentation, Comprehension, Visual fatigue, Spritz},
abstract = {In the era of small screens, traditional reading (i.e. left-to-right, top-to-bottom) is called into question and rapid serial visual presentation (RSVP) represents one of the main alternatives. RSVP consists of displaying in sequential order one or more words at a time, thus minimizing saccades and eye blinks. Recently, a RSVP application has received a lot of media attention: it is the case of Spritz. According to Spritz’s developers, the elimination of saccades should reduce visual fatigue and improve comprehension. In this study, we had people read on a computer screen a selected part of a book either with Spritz or in the traditional way. Results seem to contradict these claims. The fact that Spritz suppresses parafoveal processing and regressions (i.e. rereadings of words) negatively affected literal comprehension. Furthermore, the important reduction of eye blinks observed for Spritz might contribute to the increase of visual fatigue.}
}

@inproceedings{Sarkar2015TheIO,
  title={The impact of syntax colouring on program comprehension},
  author={Advait Sarkar},
  booktitle={PPIG},
  year={2015}
}
